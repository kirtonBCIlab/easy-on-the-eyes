{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSVEP classification comparison\n",
    "\n",
    "This notebook takes the SSVEP epochs and the no stim periods and used a classifier.\n",
    "\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "\n",
    "# Custom libraries\n",
    "from Functions import import_data\n",
    "from Functions import data_tools\n",
    "from Functions import processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and epoch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:19: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:19: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<string>:19: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:19: SyntaxWarning: invalid escape sequence '\\{'\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9428\\185781729.py:19: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  [marker_ts, markers] = import_data.read_xdf_unity_markers(f\"Data\\{file}.xdf\")\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "files = [  \n",
    "    \"sub-P002_ses-S001_task-T1_run-001_eeg\"   \n",
    "]\n",
    "ch_names = [\"O1\", \"Oz\" ,\"O2\"] # List of channel names to import\n",
    "fc = [1, 35]            # Cut-off frequencies for BW filter\n",
    "\n",
    "# Isolate subject IDs\n",
    "subject_ids = [file.split('_')[0] for file in files]\n",
    "unique_subject_ids = list(set(subject_ids))\n",
    "\n",
    "# Preallocate variables\n",
    "eeg_epochs = [None] * len(files)\n",
    "settings = [None] * len(files)\n",
    "\n",
    "for f, file in enumerate(files):\n",
    "    # Import data and markers\n",
    "    [eeg_ts, eeg_data, eeg_fs] = import_data.read_xdf(f\"Data\\\\{file}.xdf\", picks=ch_names)  \n",
    "    [marker_ts, markers] = import_data.read_xdf_unity_markers(f\"Data\\{file}.xdf\")\n",
    "\n",
    "    # Filter data\n",
    "    eeg_filt = processing.butter_filt(eeg_data, fc, \"bandpass\", eeg_fs)\n",
    "\n",
    "    # Create epochs from Unity markers\n",
    "    (eeg_epochs, epoch_labels) = data_tools.epochs_from_unity_markers(\n",
    "        eeg_time = eeg_ts,\n",
    "        eeg_data = eeg_filt,\n",
    "        marker_time = marker_ts,\n",
    "        marker_data = markers\n",
    "        )\n",
    "\n",
    "    # Fix mispelled labels & create numeric code for them\n",
    "    fixed_labels = data_tools.fix_labels(epoch_labels) \n",
    "\n",
    "    #drop resting state labels and epochs\n",
    "    labels_to_drop = [\"Resting state, eyes open\", \"Resting state, eyes closed\"]\n",
    "    drop_rs_epochs, drop_rs_labels = data_tools.drop_epochs_by_label(eeg_epochs, fixed_labels, labels_to_drop)\n",
    "\n",
    "    [label_dict, label_array] = data_tools.labels_to_dict_and_array(drop_rs_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim epochs so that they can be in numpy array instead of list\n",
    "trimmed_epochs = data_tools.trim_epochs_by_length(drop_rs_epochs)\n",
    "#time base epoch by 4 seconds \n",
    "#\"off\" = 2 4 second\n",
    "#\"on\" = 3 4 second \n",
    "\n",
    "#either 0 pad the off into 12 seconds or trim epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Riemmanian geometry classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6023054755043228"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# Create classifier\n",
    "pipelines_fb = make_pipeline(\n",
    "    Covariances(estimator=\"lwf\"),\n",
    "    TangentSpace(),\n",
    "    LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\"),\n",
    "    )\n",
    "\n",
    "# Oversample stimulus classes\n",
    "sm = ADASYN(\n",
    "    random_state = 42, n_neighbors= 2 #2 id there are 3 samples of \"on\"\n",
    ")\n",
    "[nepochs, nchans, nsamples] = np.shape(trimmed_epochs)\n",
    "reshaped_epochs = np.reshape(trimmed_epochs, [nepochs, nchans*nsamples])\n",
    "[x_oversampled, y_oversampled] = sm.fit_resample(reshaped_epochs, label_array)\n",
    "\n",
    "# - Reshape oversampled data to be [epoch, channel, samples]\n",
    "x_oversampled_unfold = np.reshape(\n",
    "    x_oversampled,\n",
    "    [x_oversampled.shape[0], nchans, -1]\n",
    "    )\n",
    "\n",
    "# Split data\n",
    "[X_train, X_test, y_train, y_test] = train_test_split(x_oversampled_unfold, y_oversampled, random_state= 4)\n",
    "\n",
    "# Test classifier\n",
    "pipelines_fb.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,\n",
       "         0,  0,  1,  1,  0,  5],\n",
       "       [ 0,  2,  0,  2,  0,  0,  0,  0,  2,  0,  0,  0,  0,  5,  0,  0,\n",
       "         4,  2,  0,  0,  1,  0],\n",
       "       [ 0,  0, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0],\n",
       "       [ 1,  3,  0,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         4,  0,  0,  0,  0,  4],\n",
       "       [ 0,  0,  0,  0, 10,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  3,  0,  2,  7,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0, 16,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 15,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  1,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0, 16,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  4,  0,  0,  0,  0,  0,  7,  0,  2,  0,  3,  1,  0,\n",
       "         1,  0,  0,  0,  0,  1],\n",
       "       [ 0,  0,  0,  1,  4,  0,  0,  0,  3,  0,  4,  0,  1,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  3,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7, 11,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0],\n",
       "       [ 4,  0,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0,  0,  4,  0,  0,\n",
       "         6,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  6,  6,\n",
       "         1,  0,  3,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 14,\n",
       "         0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  1,  0,  0,  0,  0,  0,  0,  3,  0,  0,  0,  0,  1,  0,  0,\n",
       "         7,  0,  0,  0,  0,  0],\n",
       "       [ 0,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,\n",
       "         1, 11,  0,  0,  4,  0],\n",
       "       [ 4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0, 11,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0, 13,  0,  3],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         1,  1,  0,  4, 11,  0],\n",
       "       [ 1,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  9]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pipelines_fb.predict(X_test)\n",
    "cm(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne-2023.yml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
