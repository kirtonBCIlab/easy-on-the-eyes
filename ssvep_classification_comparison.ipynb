{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSVEP classification comparison\n",
    "\n",
    "This notebook takes the SSVEP epochs and the no stim periods and used a classifier.\n",
    "\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "\n",
    "# Custom libraries\n",
    "from Functions import import_data\n",
    "from Functions import data_tools\n",
    "from Functions import processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and epoch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "files = [  \n",
    "    \"sub-P004_ses-S002_task-T1_run-002_eeg\"   \n",
    "]\n",
    "ch_names = [\"O1\", \"O2\"] # List of channel names to import\n",
    "fc = [1, 40]            # Cut-off frequencies for BW filter\n",
    "\n",
    "# Isolate subject IDs\n",
    "subject_ids = [file.split('_')[0] for file in files]\n",
    "unique_subject_ids = list(set(subject_ids))\n",
    "\n",
    "# Preallocate variables\n",
    "eeg_epochs = [None] * len(files)\n",
    "settings = [None] * len(files)\n",
    "\n",
    "for f, file in enumerate(files):\n",
    "    # Import data and markers\n",
    "    [eeg_ts, eeg_data, eeg_fs] = import_data.read_xdf(f\"Data\\\\{file}.xdf\", picks=ch_names)  \n",
    "    [marker_ts, markers] = import_data.read_xdf_unity_markers(f\"Data\\{file}.xdf\")\n",
    "\n",
    "    # Filter data\n",
    "    eeg_filt = processing.butter_filt(eeg_data, fc, \"bandpass\", eeg_fs)\n",
    "\n",
    "    # Create epochs from Unity markers\n",
    "    (eeg_epochs, epoch_labels) = data_tools.epochs_from_unity_markers(\n",
    "        eeg_time = eeg_ts,\n",
    "        eeg_data = eeg_filt,\n",
    "        marker_time = marker_ts,\n",
    "        marker_data = markers\n",
    "        )\n",
    "\n",
    "    # Fix mispelled labels & create numeric code for them\n",
    "    fixed_labels = data_tools.fix_labels(epoch_labels) \n",
    "    [label_dict, label_array] = data_tools.labels_to_dict_and_array(fixed_labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim epochs so that they can be in numpy array instead of list\n",
    "trimmed_epochs = data_tools.trim_epochs(eeg_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Riemmanian geometry classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22727272727272727"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# Create classifier\n",
    "pipelines_fb = make_pipeline(\n",
    "    Covariances(estimator=\"lwf\"),\n",
    "    TangentSpace(),\n",
    "    LogisticRegression(solver=\"lbfgs\", multi_class=\"auto\"),\n",
    "    )\n",
    "\n",
    "# Oversample stimulus classes\n",
    "sm = ADASYN(\n",
    "    random_state = 42,\n",
    "    )\n",
    "sm.n_neighbors = 3  # This is needed because if n_neighbors <= nsamples (i.e., 4) the code breaks\n",
    "[nepochs, nchans, nsamples] = np.shape(trimmed_epochs)\n",
    "reshaped_epochs = np.reshape(trimmed_epochs, [nepochs, nchans*nsamples])\n",
    "[x_oversampled, y_oversampled] = sm.fit_resample(reshaped_epochs, label_array)\n",
    "\n",
    "# - Reshape oversampled data to be [epoch, channel, samples]\n",
    "x_oversampled_unfold = np.reshape(\n",
    "    x_oversampled,\n",
    "    [x_oversampled.shape[0], nchans, -1]\n",
    "    )\n",
    "\n",
    "# Split data\n",
    "[X_train, X_test, y_train, y_test] = train_test_split(x_oversampled_unfold, y_oversampled)\n",
    "\n",
    "# Test classifier\n",
    "pipelines_fb.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 0, 0, 2, 0],\n",
       "       [0, 0, 3, 2, 0],\n",
       "       [0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [0, 0, 3, 5, 0]], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pipelines_fb.predict(X_test)\n",
    "cm(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne-2023.yml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
